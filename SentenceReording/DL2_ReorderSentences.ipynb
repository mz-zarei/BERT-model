{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL2_ReorderSentences.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UeaKM1-ZkSwe",
        "m10xqB36D_IV",
        "qCQBISp_XfFw",
        "E5b71ou1EEkY",
        "aTvWAr-0hQQL"
      ],
      "machine_shape": "hm",
      "mount_file_id": "1dxyeFqx5DtOgzokSxTrKJ5eD460hSKPO",
      "authorship_tag": "ABX9TyN4CeQaSVgzCcS4Br4793IV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3631f3062b344de3bdcc080d7d4c7c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cb360924338d48bdb5797cafdbfc6906",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2f5ec313faea416eaa335e286782db47",
              "IPY_MODEL_ca4f33845abb434c87f79cbf98faacb0"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "cb360924338d48bdb5797cafdbfc6906": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "2f5ec313faea416eaa335e286782db47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_58599bfac0d24100ac642ed2ee4cf792",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a27c3ef62ee44af48d6269106ff10c93"
          },
          "model_module_version": "1.5.0"
        },
        "ca4f33845abb434c87f79cbf98faacb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6e301b50c40347e0a38d68a64bf6f57f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 704B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_308016e5f1814dbcaa36415597aef00f"
          },
          "model_module_version": "1.5.0"
        },
        "58599bfac0d24100ac642ed2ee4cf792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "a27c3ef62ee44af48d6269106ff10c93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "6e301b50c40347e0a38d68a64bf6f57f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "308016e5f1814dbcaa36415597aef00f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "ffd6eba43cac4fe496531221dfb4c4fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_30be75bec5494fce96f23d71eca180ed",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_82de5947413f4d65ab50b9a905b6b0fe",
              "IPY_MODEL_29970ffe60654a6195d52a7c33db352d"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "30be75bec5494fce96f23d71eca180ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "82de5947413f4d65ab50b9a905b6b0fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ce9aee53f7bc4145b7413cfab21b1b0f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a14c471773114848b0c1e77117f354ee"
          },
          "model_module_version": "1.5.0"
        },
        "29970ffe60654a6195d52a7c33db352d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e5e10adaa668465386042ba7fd7a5de3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:31&lt;00:00, 14.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8adb1b9304be4d1fa157887127d243cb"
          },
          "model_module_version": "1.5.0"
        },
        "ce9aee53f7bc4145b7413cfab21b1b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "a14c471773114848b0c1e77117f354ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "e5e10adaa668465386042ba7fd7a5de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "8adb1b9304be4d1fa157887127d243cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mz-zarei/SentenceReording/blob/main/DL2_ReorderSentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeaKM1-ZkSwe"
      },
      "source": [
        "# **1. Importing required library and Unziping the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfAskS3lvhhj"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import spearmanr\n",
        "from transformers import MobileBertTokenizer, MobileBertForNextSentencePrediction\n",
        "from torch.nn.functional import softmax\n",
        "import torch\n",
        "import pickle\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from numba import jit, cuda \n",
        "import csv\n",
        "\n",
        "# Adjust the path to rar file of the data\n",
        "!unrar x \"/content/drive/MyDrive/Colab Notebooks/SentenceReordering_DataChallenge2/DataChallenge2.rar\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhz2mmlQn8UQ",
        "outputId": "79e818b8-28de-47a2-eb97-169904ffdc74"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m10xqB36D_IV"
      },
      "source": [
        "# **2. Uploading Train and Test Data** \n",
        "- Uploading data\n",
        "- Create a new data set including sentence pairs and their labels\n",
        "- Selecte a specifed number of data points from the new data set with balanced labels and considering max_words for each sentence\n",
        "- Tokenizing and saving the outputs in csv files for model training. This step takes a long time and that's why I saved them as csv files so I can use them easily. Those files are large and I couldn't uplaod them on LEARN but I can provide them if required. I used 3 sets of 1m pairs of sentences to train three models which takes a long time. But the code can be tested for 5000 pairs as well.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKS680Y91wFp"
      },
      "source": [
        "TEST_FILE_NAME = 'test.pkl'\n",
        "TRAIN_FILE_NAME = 'train.pkl'\n",
        "\n",
        "infile = open(TRAIN_FILE_NAME,'rb')\n",
        "trainset = pickle.load(infile)\n",
        "infile = open(TEST_FILE_NAME,'rb')\n",
        "testset = pickle.load(infile)\n",
        "val_data = trainset[:100]\n",
        "\n",
        "\n",
        "# path to folder with tokenized data and trained model\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/SentenceReordering_DataChallenge2/'\n",
        "\n",
        "bert_type = 'bert-base-uncased'\n",
        "max_words = 80  # max number of words allowed in a sentence in train set\n",
        "max_length = 128  # max number of tokens allowed in a sentence in train set\n",
        "batch_size = 32\n",
        "epochs = 1\n",
        "data_size = 5000   # number of data set that is used for training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZtDS2YcThDw"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# word_count = []\n",
        "# for data in testset:\n",
        "#     c = 0\n",
        "#     for i in range(6):\n",
        "#         if len(data['sentences'][i].split()) < 500:\n",
        "#             c += len(data['sentences'][i].split())\n",
        "#     word_count.append(c)\n",
        "# plt.hist(word_count, bins=100)\n",
        "# np.mean(word_count)\n",
        "\n",
        "\n",
        "# word_count = []\n",
        "# for data in testset:\n",
        "#     c = 0\n",
        "#     for i in range(6):\n",
        "#         if len(data['sentences'][i].split()) > 80:\n",
        "#             print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxik9yidUafJ"
      },
      "source": [
        "# count = 0\n",
        "# for data in testset:\n",
        "#     for i in range(6):\n",
        "#         if len(data['sentences'][i].split()) > max_words:\n",
        "#             count +=1\n",
        "# count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSFxshIJGlaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047ee78b-7a41-419d-d95e-a2eaf46611fa"
      },
      "source": [
        "# Make a new data set using each pair of 6 given sentence in each data point (30 pairs),\n",
        "# 25 pairs with label 1 and 5 pair with label 0.\n",
        "\n",
        "pairs = list(itertools.permutations(range(6),2))\n",
        "data_set = []\n",
        "# trainset = trainset[0:3]\n",
        "for ID in tqdm(range(len(trainset[:data_size]))):\n",
        "    \n",
        "    sentences = trainset[ID]['sentences']\n",
        "    indexes = trainset[ID]['indexes']\n",
        "    for p in pairs:\n",
        "        if indexes[p[1]] == indexes[p[0]] + 1:\n",
        "            data = [sentences [p[0]],sentences [p[1]], 0]  # sentence A, sentence B, lable (0: B is a continuation of sequence A, 1:B is a random sequence)\n",
        "            data_set.append(data)\n",
        "        else:\n",
        "            data_set.append([sentences [p[0]],sentences [p[1]], 1] )\n",
        "\n",
        "random.shuffle(data_set)   # shuffling the new data set\n",
        "print(len(trainset))\n",
        "print(len(data_set))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 5000/5000 [00:00<00:00, 42978.74it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "590226\n",
            "150000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ0W1MkILQr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db4fcc9b-452d-44b1-a274-1e66c9ada733"
      },
      "source": [
        "data_balanced = []   # 1m balanced data which has max_words are selected for tokenization\n",
        "count_1 = 0\n",
        "count_0 = 0\n",
        "for data in tqdm(data_set):\n",
        "    if len(data[0].split()) < max_words and len(data[1].split()) < max_words:\n",
        "        if data[2] == 0:\n",
        "            if count_0 < data_size/2:\n",
        "                count_0 += 1\n",
        "                data_balanced.append(data)\n",
        "        elif count_1 < data_size/2:\n",
        "            count_1 += 1\n",
        "            data_balanced.append(data)\n",
        "\n",
        "del data_set\n",
        "del trainset\n",
        "print(len(data_balanced))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/150000 [00:00<?, ?it/s]\u001b[A\n",
            " 17%|█▋        | 25424/150000 [00:00<00:00, 254225.70it/s]\u001b[A\n",
            " 35%|███▍      | 51997/150000 [00:00<00:00, 257569.84it/s]\u001b[A\n",
            " 52%|█████▏    | 77577/150000 [00:00<00:00, 257034.92it/s]\u001b[A\n",
            " 70%|██████▉   | 104420/150000 [00:00<00:00, 260349.18it/s]\u001b[A\n",
            "100%|██████████| 150000/150000 [00:00<00:00, 263521.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KQzdGeiP3EY"
      },
      "source": [
        "# count_words = 0\n",
        "# count_sentence = 0\n",
        "\n",
        "# for data in tqdm(data_set):\n",
        "#     if len(data[0].split()) > 50 :\n",
        "#         count_words += 1\n",
        "# count_words       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC-MiOmUdNXb"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
        "\n",
        "\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "token_type_ids =[]\n",
        "labels = []\n",
        "\n",
        "# For every data point...\n",
        "for data in tqdm(data_balanced):\n",
        "\n",
        "    s1, s2, label = data[0], data[1], data[2]\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        s1, text_pair = s2,                      # Two sentences to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = max_length,          # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',           # Return pytorch tensors.\n",
        "                        truncation= True     \n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "    # And its token type ids (which sentence is first).\n",
        "    token_type_ids.append(encoded_dict['token_type_ids'])\n",
        "    # Get labels\n",
        "    labels.append(label)\n",
        "\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "token_type_ids = torch.cat(token_type_ids, dim=0)\n",
        "labels = torch.Tensor(labels)\n",
        "\n",
        " \n",
        "# Save csv files of tokenized data\n",
        "input_ids_df = pd.DataFrame(input_ids.numpy())\n",
        "input_ids_df.to_csv(path + 'input_ids_new.csv', index=False)\n",
        "\n",
        "attention_masks_df = pd.DataFrame(attention_masks.numpy())\n",
        "attention_masks_df.to_csv(path + 'attention_masks_new.csv', index=False)\n",
        "\n",
        "token_type_ids_df = pd.DataFrame(token_type_ids.numpy())\n",
        "token_type_ids_df.to_csv(path + 'token_type_ids_new.csv', index=False)\n",
        "\n",
        "labels_df = pd.DataFrame(labels.numpy())\n",
        "labels_df.to_csv(path + 'labels_new.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "# # Print sentence 0, now as a list of IDs.\n",
        "# print('Original: ', data_balanced[0])\n",
        "# print('Token IDs:', input_ids[0])\n",
        "# print('Token type IDs:', token_type_ids[0])\n",
        "# print('label:', labels[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCQBISp_XfFw"
      },
      "source": [
        "# **3. Finetunning BertForNextSentencePrediction**\n",
        "In this part a Bert Model trained using the balanced data set from previous step. This is mainly based on tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z0DAu92ddjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9ad3b56-777c-4d15-916d-3f379592da1a"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "\n",
        "input_ids_df = pd.read_csv(path + 'input_ids_new.csv')\n",
        "input_ids = torch.Tensor(input_ids_df.values[:data_size]).to(torch.int64)\n",
        "\n",
        "attention_masks_df = pd.read_csv(path + 'attention_masks_new.csv')\n",
        "attention_masks = torch.Tensor(attention_masks_df.values[:data_size]).to(torch.int64)\n",
        "\n",
        "token_type_ids_df = pd.read_csv(path + 'token_type_ids_new.csv')\n",
        "token_type_ids = torch.Tensor(token_type_ids_df.values[:data_size]).to(torch.int64)\n",
        "\n",
        "labels_df = pd.read_csv(path + 'labels_new.csv').T\n",
        "labels = torch.Tensor(labels_df.values[0][:data_size]).to(torch.int64)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "# print('Original: ', data_balanced[0])\n",
        "# print('Token IDs:', input_ids[0])\n",
        "# print('Token type IDs:', token_type_ids[0])\n",
        "# print('label:', labels[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4,500 training samples\n",
            "  500 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogR2A-oEi5pG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193,
          "referenced_widgets": [
            "3631f3062b344de3bdcc080d7d4c7c48",
            "cb360924338d48bdb5797cafdbfc6906",
            "2f5ec313faea416eaa335e286782db47",
            "ca4f33845abb434c87f79cbf98faacb0",
            "58599bfac0d24100ac642ed2ee4cf792",
            "a27c3ef62ee44af48d6269106ff10c93",
            "6e301b50c40347e0a38d68a64bf6f57f",
            "308016e5f1814dbcaa36415597aef00f",
            "ffd6eba43cac4fe496531221dfb4c4fe",
            "30be75bec5494fce96f23d71eca180ed",
            "82de5947413f4d65ab50b9a905b6b0fe",
            "29970ffe60654a6195d52a7c33db352d",
            "ce9aee53f7bc4145b7413cfab21b1b0f",
            "a14c471773114848b0c1e77117f354ee",
            "e5e10adaa668465386042ba7fd7a5de3",
            "8adb1b9304be4d1fa157887127d243cb"
          ]
        },
        "outputId": "c29d3693-77b2-466e-e67e-ba8240fd243c"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForNextSentencePrediction, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        " \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "model = BertForNextSentencePrediction.from_pretrained(\n",
        "    bert_type, \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3631f3062b344de3bdcc080d7d4c7c48",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffd6eba43cac4fe496531221dfb4c4fe",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-42YUxa5MpW"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBpoN5gL5Q_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "021c3e00-f3b6-4f5c-c4aa-e14045bbb350"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "seed_val = 42 \n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 10 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_input_type = batch[2].to(device)\n",
        "        b_label = batch[3].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        result = model(b_input_ids, \n",
        "                       token_type_ids=b_input_type, \n",
        "                       attention_mask=b_input_mask, \n",
        "                       labels=b_label,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_input_type = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=b_input_type, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "            \n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"\")\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "    \n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n",
            "  Batch    50  of    141.    Elapsed: 0:00:20.\n",
            "  Batch   100  of    141.    Elapsed: 0:00:39.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epcoh took: 0:00:55\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Accuracy: 0.72\n",
            "  Validation Loss: 0.55\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:57 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDF95v4iz1dX"
      },
      "source": [
        "\n",
        "torch.save({'model' : model,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_train_loss,\n",
        "            }, path + 'checkpoint-new.pt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5b71ou1EEkY"
      },
      "source": [
        "# **4. Predicting Order of Test Data With Three Models**\n",
        "I used three trained BERT models using the given data set with different portion of data each time. I couldn't upload three models in LEARN as they have about 400mb size each, but I can share a link to Google Drive if required. Then I used all three outputs from each model for scoring 720 permutations of the given 6 sentences in the Testset and the order with maximum score (sum of 5 scores regarding 5 pairs of a given order) is selected as the best order for 6 sentences. This process takes about an hour with Colab Pro. For example fo an order of the given 6 setences, [s1,s3,s0,s4,s2,s5] the score from one BERT model is computed as follows BERT[s1,s3]+BERT[s3,s0]+BERT[s0,s4]+BERT[s4,s2]+BERT[s2,s5]. If we use only one of the models the Spearman's test becomes around 0.80 but if we use all three models in a bagging form it improves to 0.812 for the test and validation set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVF8UefI-QHt"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "## Loading three trained BEertForNextSentencePrediction models\n",
        "model1 = torch.load('/content/model1.pt')\n",
        "model2 = torch.load('/content/model2.pt')\n",
        "model3 = torch.load('/content/model3.pt')\n",
        "\n",
        "\n",
        "model1.to(device)\n",
        "model1.eval()\n",
        "model2.to(device)\n",
        "model2.eval()\n",
        "model3.to(device)\n",
        "model3.eval()\n",
        "\n",
        "test_order_list = []   # this is final answer\n",
        "\n",
        "# this function calculates the score of a given pair using three BERT models that I trained using the given data set in previous steps\n",
        "def score_pair(s1,s2):\n",
        "    encoding = tokenizer.encode_plus(s1, text_pair = s2, return_tensors='pt',max_length=max_length, pad_to_max_length = True, add_special_tokens= True).to(device)\n",
        "    input_ids = encoding['input_ids']\n",
        "    attention_masks = encoding['attention_mask']\n",
        "    token_type_ids = encoding['token_type_ids']\n",
        "\n",
        "    outputs1 = model1(**encoding)\n",
        "    sx1 = softmax(outputs1.logits, dim = 1)\n",
        "    score1 = sx1.tolist()[0][0]\n",
        "\n",
        "    outputs2 = model2(**encoding)\n",
        "    sx2 = softmax(outputs2.logits, dim = 1)\n",
        "    score2 = sx2.tolist()[0][0]\n",
        "\n",
        "    outputs3 = model3(**encoding)\n",
        "    sx3 = softmax(outputs3.logits, dim = 1)\n",
        "    score3 = sx3.tolist()[0][0]\n",
        "\n",
        "\n",
        "    return  score1 + score1 + score3\n",
        "\n",
        "\n",
        "all_pairs = list(itertools.permutations([0,1,2,3,4,5],2))\n",
        "orders = list(itertools.permutations([0,1,2,3,4,5]))\n",
        "\n",
        "for ID in tqdm(range(len(testset))):\n",
        "    sentences = testset[ID]\n",
        "\n",
        "    pair_score_dict = {} # the BERT score for each pair of sentences\n",
        "\n",
        "    # computing each pair score\n",
        "    for pair in all_pairs:\n",
        "        s1 = sentences['sentences'][pair[0]]\n",
        "        s2 = sentences['sentences'][pair[1]]\n",
        "\n",
        "        pair_score_dict[pair] = score_pair(s1, s2)\n",
        "\n",
        "\n",
        "    score = 0 # best score of orders\n",
        "    best_ord = []\n",
        "    for ord in orders:\n",
        "        pairs = [(ord.index(0),ord.index(1)),\n",
        "                 (ord.index(1),ord.index(2)),\n",
        "                 (ord.index(2),ord.index(3)),\n",
        "                 (ord.index(3),ord.index(4)),\n",
        "                 (ord.index(4),ord.index(5))]\n",
        "        score_ord = 0  # score of the ord\n",
        "        for pair in pairs:\n",
        "            score_ord += pair_score_dict[pair]\n",
        "        if score_ord > score:\n",
        "            best_ord = ord\n",
        "            score = score_ord\n",
        "\n",
        "    test_order_list.append(best_ord)\n",
        "\n",
        "pd.DataFrame(test_order_list).to_excel('test.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTvWAr-0hQQL"
      },
      "source": [
        "# **5. Predicting Order of Validation Data With One Model**\n",
        "Here the trained models can be tested on validation set. With just 5000 data you should get about 0.7 for Spearman test on the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-A0zj0u8x2Y"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "## Loading three trained BEertForNextSentencePrediction models\n",
        "model = torch.load(path + 'checkpoint-new.pt')['model']\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "val_order_list = []   \n",
        "sr = []      # Spearsman score list\n",
        "\n",
        "def score_pair(s1,s2):\n",
        "    encoding = tokenizer.encode_plus(s1, text_pair = s2, return_tensors='pt',max_length=max_length, pad_to_max_length = True, add_special_tokens= True).to(device)\n",
        "    input_ids = encoding['input_ids']\n",
        "    attention_masks = encoding['attention_mask']\n",
        "    token_type_ids = encoding['token_type_ids']\n",
        "\n",
        "    outputs = model(**encoding)\n",
        "    sx = softmax(outputs.logits, dim = 1)\n",
        "    score = sx.tolist()[0][0]\n",
        "\n",
        "    return  score\n",
        "\n",
        "\n",
        "all_pairs = list(itertools.permutations([0,1,2,3,4,5],2))\n",
        "orders = list(itertools.permutations([0,1,2,3,4,5]))\n",
        "\n",
        "\n",
        "for ID in tqdm(range(len(val_data))):\n",
        "    sentences = val_data[ID]\n",
        "    \n",
        "\n",
        "    pair_score_dict = {} # the BERT score for each pair of sentences\n",
        "\n",
        "    # computing each pair score\n",
        "    for pair in all_pairs:\n",
        "        s1 = sentences['sentences'][pair[0]]\n",
        "        s2 = sentences['sentences'][pair[1]]\n",
        "\n",
        "        pair_score_dict[pair] = score_pair(s1, s2)\n",
        "\n",
        "\n",
        "    score = 0 # best score of orders\n",
        "    best_ord = []\n",
        "    for ord in orders:\n",
        "        pairs = [(ord.index(0),ord.index(1)),\n",
        "                 (ord.index(1),ord.index(2)),\n",
        "                 (ord.index(2),ord.index(3)),\n",
        "                 (ord.index(3),ord.index(4)),\n",
        "                 (ord.index(4),ord.index(5))]\n",
        "        score_ord = 0  # score of the ord\n",
        "        for pair in pairs:\n",
        "            score_ord += pair_score_dict[pair]\n",
        "        if score_ord > score:\n",
        "            best_ord = ord\n",
        "            score = score_ord\n",
        "    sr.append(spearmanr(sentences['indexes'], best_ord)[0])\n",
        "    val_order_list.append(best_ord)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(np.mean(sr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DN_Q1PE1l5H"
      },
      "source": [
        "# **6. Predicting Order of Test Data With One Model**\n",
        "Here the trained model or the submitted model trained on 1m data (model3.pt in the submission folder) can be used on test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYFABfn1kXu"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "## Loading three trained BEertForNextSentencePrediction models\n",
        "\n",
        "# model = torch.load(path + 'checkpoint-new.pt')['model']\n",
        "model = torch.load(path + 'model3.pt')\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "test_order_list = []   \n",
        "sr = []      # Spearsman score list\n",
        "\n",
        "def score_pair(s1,s2):\n",
        "    encoding = tokenizer.encode_plus(s1, text_pair = s2, return_tensors='pt',max_length=max_length, pad_to_max_length = True, add_special_tokens= True).to(device)\n",
        "    input_ids = encoding['input_ids']\n",
        "    attention_masks = encoding['attention_mask']\n",
        "    token_type_ids = encoding['token_type_ids']\n",
        "\n",
        "    outputs = model(**encoding)\n",
        "    sx = softmax(outputs.logits, dim = 1)\n",
        "    score = sx.tolist()[0][0]\n",
        "\n",
        "    return  score\n",
        "\n",
        "\n",
        "all_pairs = list(itertools.permutations([0,1,2,3,4,5],2))\n",
        "orders = list(itertools.permutations([0,1,2,3,4,5]))\n",
        "\n",
        "\n",
        "for ID in tqdm(range(len(testset))):\n",
        "    sentences = testset[ID]\n",
        "    \n",
        "\n",
        "    pair_score_dict = {} # the BERT score for each pair of sentences\n",
        "\n",
        "    # computing each pair score\n",
        "    for pair in all_pairs:\n",
        "        s1 = sentences['sentences'][pair[0]]\n",
        "        s2 = sentences['sentences'][pair[1]]\n",
        "\n",
        "        pair_score_dict[pair] = score_pair(s1, s2)\n",
        "\n",
        "\n",
        "    score = 0 # best score of orders\n",
        "    best_ord = []\n",
        "    for ord in orders:\n",
        "        pairs = [(ord.index(0),ord.index(1)),\n",
        "                 (ord.index(1),ord.index(2)),\n",
        "                 (ord.index(2),ord.index(3)),\n",
        "                 (ord.index(3),ord.index(4)),\n",
        "                 (ord.index(4),ord.index(5))]\n",
        "        score_ord = 0  # score of the ord\n",
        "        for pair in pairs:\n",
        "            score_ord += pair_score_dict[pair]\n",
        "        if score_ord > score:\n",
        "            best_ord = ord\n",
        "            score = score_ord\n",
        "    test_order_list.append(best_ord)\n",
        "\n",
        "pd.DataFrame(test_order_list).to_excel('test.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB9PqT4z2FGt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
